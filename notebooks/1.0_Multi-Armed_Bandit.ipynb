{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple version of the k-armed bandit problem is useful because of its nonassociative nature. This is a good problem or environement to learn basic reinforcement learning methodes.\n",
    "\n",
    "So let's create a simple gymnasium environement to re-create the k-armed bandit problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The k-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are faced repeatedly with a choice among\n",
    "k di↵erent options, or actions. After each choice you receive a numerical reward chosen\n",
    "from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example,\n",
    "over 1000 action selections, or time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit(gym.Env):\n",
    "\n",
    "    def __init__(self, nb_arms=10, nb_steps=1000):\n",
    "        self._nb_arms = nb_arms\n",
    "        self._nb_steps = nb_steps\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(nb_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._step += 1\n",
    "    \n",
    "        reward = self._arms[action]\n",
    "        reward_noise = jax.random.normal(jax.random.key(self._step), (1))[0] # self.np_random.normal(0, 1, size=1)[0]\n",
    "        terminated = self._step >= self._nb_steps\n",
    "\n",
    "        info = { \"is_optimal_action\": int(action == np.argmax(self._arms)) }\n",
    "\n",
    "        return reward + reward_noise, terminated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._step = 0\n",
    "        self._arms = self.np_random.normal(0, 1, size=self._nb_arms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see if the implementation of the k-armed bandit is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KArmedBandit(nb_arms=10)\n",
    "env.reset()\n",
    "\n",
    "# Sample our distribution to see it's correct\n",
    "data = np.array([[env.step(i)[0] for i in range(len(env._arms))] for _ in range(100)])\n",
    "index = [i for i in range(len(env._arms))]\n",
    "\n",
    "plt.violinplot(data, index, showmeans=True)\n",
    "\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems good !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.sum_of_rewards = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.sum_of_rewards / self.nb_action_taken)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.sum_of_rewards[action] += reward\n",
    "        self.nb_action_taken[action] += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.sum_of_rewards = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \"-greedy action selection, for the case of two actions and \" = 0.5, what is\n",
    "the probability that the greedy action is selected?\n",
    "\n",
    "Well, there is a probability of 0.5 to take the greedy action then 0.5 to take a random action; in this case there is a 1/2 chance to take the greedy action.\n",
    "So the answer is 0.5 + (0.5 * 0.5) = 0.75 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KArmedBandit(nb_arms=10)\n",
    "agent = EpsilonGreedy(nb_actions=10, epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent):\n",
    "    list_of_reward = []\n",
    "    list_of_optimal_action = []\n",
    "\n",
    "    env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action()\n",
    "\n",
    "        reward, terminated, info = env.step(action)\n",
    "\n",
    "        agent.observe(action, reward)\n",
    "\n",
    "        list_of_reward.append(reward)\n",
    "        list_of_optimal_action.append(info[\"is_optimal_action\"])\n",
    "    \n",
    "    return np.array(list_of_reward), np.array(list_of_optimal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reward, list_of_optimal_action = run_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_of_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_of_optimal_action)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to see any result here, there is to much noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this for 2000 independent runs,\n",
    "each with a di↵erent bandit problem, we obtained measures of the learning algorithm’s\n",
    "average behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(nb_exps, env, agent):\n",
    "    list_rewards, list_optimal_action = run_env(env, agent)\n",
    "\n",
    "    for _ in range(nb_exps - 1):\n",
    "        list_rewards_tmp, list_optimal_action_tmp = run_env(env, agent)\n",
    "\n",
    "        list_rewards += list_rewards_tmp\n",
    "        list_optimal_action += list_optimal_action_tmp\n",
    "\n",
    "    return list_rewards / nb_exps, (list_optimal_action / nb_exps) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KArmedBandit(nb_arms=10)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_01 = EpsilonGreedy(nb_actions=10, epsilon=0.01)\n",
    "mean_rewards_01, percent_optimal_action_01 = run_exp(2000, env, agent_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = EpsilonGreedy(nb_actions=10, epsilon=0.1)\n",
    "mean_rewards_1, percent_optimal_action_1 = run_exp(2000, env, agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_greedy = EpsilonGreedy(nb_actions=10, epsilon=0.0)\n",
    "mean_rewards_0, percent_optimal_action_0 = run_exp(2000, env, agent_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards_01, color=\"tab:red\", label='Egreedy 0.01')\n",
    "\n",
    "plt.plot(mean_rewards_1, color=\"tab:blue\", label='Egreedy 0.1')\n",
    "\n",
    "plt.plot(mean_rewards_0, color=\"tab:green\", label='Egreedy 0.0 (greedy)')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(percent_optimal_action_01, color=\"tab:red\", label='Egreedy 0.01')\n",
    "\n",
    "plt.plot(percent_optimal_action_1, color=\"tab:blue\", label='Egreedy 0.1')\n",
    "\n",
    "plt.plot(percent_optimal_action_0, color=\"tab:green\", label='Egreedy 0.0 (greedy)')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('% Optimal Action')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "        self.q[action] += (reward - self.q[action]) / self.nb_action_taken[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KArmedBandit(nb_arms=10)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_01 = EpsilonGreedy(nb_actions=10, epsilon=0.01)\n",
    "mean_rewards_01, percent_optimal_action_01 = run_exp(2000, env, agent_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = EpsilonGreedy(nb_actions=10, epsilon=0.1)\n",
    "mean_rewards_1, percent_optimal_action_1 = run_exp(2000, env, agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_greedy = EpsilonGreedy(nb_actions=10, epsilon=0.0)\n",
    "mean_rewards_0, percent_optimal_action_0 = run_exp(2000, env, agent_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards_01, color=\"tab:red\", label='Egreedy 0.01')\n",
    "\n",
    "plt.plot(mean_rewards_1, color=\"tab:blue\", label='Egreedy 0.1')\n",
    "\n",
    "plt.plot(mean_rewards_0, color=\"tab:green\", label='Egreedy 0.0 (greedy)')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(percent_optimal_action_01, color=\"tab:red\", label='Egreedy 0.01')\n",
    "\n",
    "plt.plot(percent_optimal_action_1, color=\"tab:blue\", label='Egreedy 0.1')\n",
    "\n",
    "plt.plot(percent_optimal_action_0, color=\"tab:green\", label='Egreedy 0.0 (greedy)')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('% Optimal Action')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon, alpha, optimistic_value=0):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.optimistic_value = optimistic_value\n",
    "\n",
    "        self.q = np.ones(self.nb_actions) * optimistic_value\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.q[action] += self.alpha * (reward - self.q[action])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.ones(self.nb_actions) * self.optimistic_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KArmedBandit(nb_arms=10)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_optimistic = EpsilonGreedy(nb_actions=10, epsilon=0.01, alpha=0.1, optimistic_value=5)\n",
    "mean_rewards_optimistic, percent_optimal_action_optimistic = run_exp(2000, env, agent_optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_non_otpimistic = EpsilonGreedy(nb_actions=10, epsilon=0.01, alpha=0.1, optimistic_value=0)\n",
    "mean_rewards_non_optimistic, percent_optimal_action_non_optimistic = run_exp(2000, env, agent_non_otpimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards_optimistic, color=\"tab:red\", label='Egreedy Optimistic')\n",
    "\n",
    "plt.plot(mean_rewards_non_optimistic, color=\"tab:blue\", label='Egreedy Non Optimistic')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(percent_optimal_action_optimistic, color=\"tab:red\", label='Egreedy Optimistic')\n",
    "\n",
    "plt.plot(percent_optimal_action_non_optimistic, color=\"tab:blue\", label='Egreedy Non Optimistic')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper-Confidence-Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfidenceBound():\n",
    "\n",
    "    def __init__(self, nb_actions, confidence, alpha):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.confidence = confidence\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "        self.upper_configdence = np.zeros(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        return np.argmax(self.q + self.upper_configdence)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.q[action] += self.alpha * (reward - self.q[action])\n",
    "\n",
    "        self.nb_action_taken[action] += 1\n",
    "\n",
    "        self.upper_configdence = self.confidence * np.sqrt(np.log(np.sum(self.nb_action_taken)) / self.nb_action_taken)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "        self.upper_configdence = np.zeros(self.nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KArmedBandit(nb_arms=10)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_condidence_agent = UpperConfidenceBound(nb_actions=10, confidence=2, alpha=0.1)\n",
    "mean_rewards_upper_confidence, percent_optimal_action_upper_confidence = run_exp(2000, env, upper_condidence_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egreedy_agent = EpsilonGreedy(nb_actions=10, epsilon=0.01, alpha=0.1, optimistic_value=0)\n",
    "mean_rewards_egreedy, percent_optimal_action_egreedy = run_exp(2000, env, egreedy_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards_upper_confidence, color=\"tab:blue\", label='Egreedy Optimistic')\n",
    "\n",
    "plt.plot(mean_rewards_egreedy, color=\"tab:gray\", label='Egreedy Non Optimistic')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

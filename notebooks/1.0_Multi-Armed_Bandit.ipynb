{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "RL uses training information that evaluates the actions taken ratther than instructs by giving correct actions.\n",
    "This is what creates the need for active exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "We will use:\n",
    "- Numpy for calculation\n",
    "- gymnasium for rl reproducible envrionment\n",
    "- plotly for graph ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-armed Bandit Problem\n",
    "\n",
    "A simple version of the k-armed bandit problem is useful because of its nonassociative nature. This is a good problem or environement to learn basic reinforcement learning methodes since it avoids much of the complexity of the full reinforcement learning.\n",
    "\n",
    "So let's create a simple gymnasium environement to re-create the k-armed bandit problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The k-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are faced repeatedly with a choice among\n",
    "k different options, or actions. After each choice you receive a numerical reward chosen\n",
    "from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example,\n",
    "over 1000 action selections, or time steps.\n",
    "\n",
    "Here we use gymnasium for craeting what we call an environement. Gymnasium is an framework that allow to standardize the environment api so that poeple can easily reproduce experimentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gym](https://github.com/openai/gym) is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this.\n",
    "\n",
    "[Gymnasium](https://gymnasium.farama.org/) is a maintained fork of OpenAI’s Gym library. The Gymnasium interface is simple, pythonic, and capable of representing general RL problems, and has a compatibility wrapper for old Gym environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-armed bandit problem implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit(gym.Env):\n",
    "\n",
    "    def __init__(self, nb_arms=10, nb_steps=1000, mean=0, variance=1, noise_variance=1):\n",
    "        self._nb_arms = nb_arms\n",
    "        self._nb_steps = nb_steps\n",
    "\n",
    "        self._mean = 0\n",
    "        self._noise_mean = 0\n",
    "        self._variance = variance\n",
    "        self._noise_variance = noise_variance\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(nb_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._step += 1\n",
    "    \n",
    "        reward = self._arms[action]\n",
    "        reward_noise = self.np_random.normal(self._noise_mean, self._noise_variance)\n",
    "        terminated = self._step >= self._nb_steps\n",
    "\n",
    "        info = { \"is_optimal_action\": int(action == np.argmax(self._arms)) }\n",
    "\n",
    "        return reward + reward_noise, terminated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._step = 0\n",
    "        self._arms = self.np_random.normal(self._mean, self._variance, size=self._nb_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(main_title, titles, results):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=titles)\n",
    "\n",
    "    for result_name in results:\n",
    "        x = np.arange(len(results[result_name][\"mean_reward\"]))\n",
    "        fig.add_trace(go.Scatter(x=x, y=results[result_name][\"mean_reward\"], line_color=results[result_name][\"color\"], name=result_name), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=x, y=results[result_name][\"optimal_action\"], line_color=results[result_name][\"color\"], name=result_name, showlegend=False), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=main_title,\n",
    "        legend_title=\"Parameters\",\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing gym env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_arms  = 10\n",
    "nb_steps = 1000\n",
    "\n",
    "env = KArmedBandit(nb_arms=nb_arms, nb_steps=nb_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see if the implementation of the k-armed bandit is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# Sample our distribution to see it's correct\n",
    "data = np.array([[env.step(i)[0] for _ in range(2000)] for i in range(len(env._arms))])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(env._arms)):\n",
    "    fig.add_trace(go.Violin(x=[i] * len(data[i]), y=data[i], name=\"q*(\" + str(i) + \") = \" + str(env._arms[i])[:4], meanline_visible=True))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"K-Armed Bandit Problem Distribution\",\n",
    "    xaxis_title=\"Actions\",\n",
    "    yaxis_title=\"Reward Distributions\",\n",
    "    legend_title=\"True Value of q*(a)\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true value of each q*(a) is near the mean of each distribution, it seems good !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_*(a) = \\Epsilon[R_t | A_t = a]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "        self.q[action] += (reward - self.q[action]) / self.nb_action_taken[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \"-greedy action selection, for the case of two actions and \" = 0.5, what is\n",
    "the probability that the greedy action is selected?\n",
    "\n",
    "Well, there is a probability of 0.5 to take the greedy action then 0.5 to take a random action; in this case there is a 1/2 chance to take the greedy action.\n",
    "So the answer is 0.5 + (0.5 * 0.5) = 0.75 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = EpsilonGreedy(nb_actions=10, epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent):\n",
    "    list_of_reward = []\n",
    "    list_of_optimal_action = []\n",
    "\n",
    "    env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action()\n",
    "\n",
    "        reward, terminated, info = env.step(action)\n",
    "\n",
    "        agent.observe(action, reward)\n",
    "\n",
    "        list_of_reward.append(reward)\n",
    "        list_of_optimal_action.append(info[\"is_optimal_action\"])\n",
    "    \n",
    "    return np.array(list_of_reward), np.array(list_of_optimal_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running first experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reward, list_of_optimal_action = run_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(list_of_reward)), y=list_of_reward, mode='lines'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(list_of_optimal_action)), y=list_of_optimal_action, mode='lines', name='Canada'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to see any result here, there is to much noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this for 2000 independent runs,\n",
    "each with a different bandit problem, we obtained measures of the learning algorithm’s\n",
    "average behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(nb_exps, env, agent):\n",
    "    list_rewards, list_optimal_action = run_env(env, agent)\n",
    "\n",
    "    for _ in range(nb_exps - 1):\n",
    "        list_rewards_tmp, list_optimal_action_tmp = run_env(env, agent)\n",
    "\n",
    "        list_rewards += list_rewards_tmp\n",
    "        list_optimal_action += list_optimal_action_tmp\n",
    "\n",
    "    return list_rewards / nb_exps, (list_optimal_action / nb_exps) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_01 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.01)\n",
    "mean_rewards_01, percent_optimal_action_01 = run_exp(nb_exps, env, agent_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1)\n",
    "mean_rewards_1, percent_optimal_action_1 = run_exp(nb_exps, env, agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_greedy = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.0)\n",
    "mean_rewards_0, percent_optimal_action_0 = run_exp(nb_exps, env, agent_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp_1 = {\n",
    "    'Egreedy 0.01': {\n",
    "        \"mean_reward\": mean_rewards_01,\n",
    "        \"optimal_action\": percent_optimal_action_01,\n",
    "        \"color\": \"red\"\n",
    "    },\n",
    "    'Egreedy 0.1': {\n",
    "        \"mean_reward\": mean_rewards_1,\n",
    "        \"optimal_action\": percent_optimal_action_1,\n",
    "        \"color\": \"blue\"\n",
    "    },\n",
    "    'Greedy (0.0)': {\n",
    "        \"mean_reward\": mean_rewards_0,\n",
    "        \"optimal_action\": percent_optimal_action_0,\n",
    "        \"color\": \"green\"\n",
    "    },\n",
    "}\n",
    "\n",
    "plot_results(\n",
    "    \"Compares greedy method with different parameters (0.01, 0.1 and 0)\",\n",
    "    [\"Average Reward / Steps\", \"Optimal Action / Steps\"],\n",
    "    results_exp_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon=0.1, optimistic_value=0):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.optimistic_value = optimistic_value\n",
    "\n",
    "        self.nb_times_taken_action = np.zeros(self.nb_actions)\n",
    "        self.q = np.ones(self.nb_actions) * optimistic_value\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_times_taken_action[action] += 1\n",
    "        self.q[action] += 0.1 * (reward - self.q[action]) # / self.nb_times_taken_action[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.nb_times_taken_action = np.zeros(self.nb_actions)\n",
    "        self.q = np.ones(self.nb_actions) * self.optimistic_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_optimistic_greedy = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0, optimistic_value=5)\n",
    "mean_rewards_optimistic, percent_optimal_action_optimistic = run_exp(nb_exps, env, agent_optimistic_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_non_optimistic = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1, optimistic_value=0)\n",
    "mean_rewards_non_optimistic, percent_optimal_action_non_optimistic = run_exp(nb_exps, env, agent_non_optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp_2 = {\n",
    "    'Greedy Optimistic': {\n",
    "        \"mean_reward\": mean_rewards_optimistic,\n",
    "        \"optimal_action\": percent_optimal_action_optimistic,\n",
    "        \"color\": \"blue\"\n",
    "    },\n",
    "    'Egreedy Non Optimistic': {\n",
    "        \"mean_reward\": mean_rewards_non_optimistic,\n",
    "        \"optimal_action\": percent_optimal_action_non_optimistic,\n",
    "        \"color\": \"red\"\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_results(\"Optimistic greedy vs Non optimistic 0.01\", [\"Average Reward / Steps\", \"Optimal Action / Steps\"], results_exp_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper-Confidence-Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfidenceBound():\n",
    "\n",
    "    def __init__(self, nb_actions, confidence, alpha=0.1):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.confidence = confidence\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "        self.upper_configdence = np.ones(self.nb_actions) * np.inf\n",
    "\n",
    "    def action(self):\n",
    "        return np.argmax(self.q + self.upper_configdence)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "\n",
    "        self.q[action] += (reward - self.q[action]) / self.nb_action_taken[action]\n",
    "\n",
    "        if not 0 in self.nb_action_taken:\n",
    "            self.upper_configdence = self.confidence * np.sqrt(np.log(np.sum(self.nb_action_taken)) / self.nb_action_taken)\n",
    "        else:\n",
    "            self.upper_configdence[action] = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.zeros(self.nb_actions)\n",
    "        self.upper_configdence = np.ones(self.nb_actions) * np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_condidence_agent = UpperConfidenceBound(nb_actions=env.action_space.n, confidence=2, alpha=0.1)\n",
    "mean_rewards_upper_confidence, percent_optimal_action_upper_confidence = run_exp(nb_exps, env, upper_condidence_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "egreedy_agent = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1, optimistic_value=0)\n",
    "mean_rewards_egreedy, percent_optimal_action_egreedy = run_exp(nb_exps, env, egreedy_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp_3 = {\n",
    "    'UCB': {\n",
    "        \"mean_reward\": mean_rewards_upper_confidence,\n",
    "        \"optimal_action\": percent_optimal_action_upper_confidence,\n",
    "        \"color\": \"blue\"\n",
    "    },\n",
    "    'Egreedy 0.1': {\n",
    "        \"mean_reward\": mean_rewards_egreedy,\n",
    "        \"optimal_action\": percent_optimal_action_egreedy,\n",
    "        \"color\": \"red\"\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_results(\"Upper Confidence Bound vs Epsilon Greedy\", [\"Average Reward / Steps\", \"Optimal Action / Steps\"], results_exp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Bandit Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(state_action_value):\n",
    "    e_x = np.exp(state_action_value - np.max(state_action_value))\n",
    "    probs = e_x / e_x.sum(axis=0)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBandit():\n",
    "\n",
    "    def __init__(self, nb_actions, alpha):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.alpha = alpha\n",
    "        self.soft_probs = None\n",
    "        self.nb_action_taken = 0\n",
    "\n",
    "        self.mean_reward = 0\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        self.soft_probs = softmax(self.q)\n",
    "        return np.random.choice(self.nb_actions, 1, p=self.soft_probs)[0]\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken += 1\n",
    "        self.mean_reward += (reward - self.mean_reward) / self.nb_action_taken\n",
    "\n",
    "        self.soft_probs[action] = - (1 - self.soft_probs[action])\n",
    "        self.q -= self.alpha * (reward - self.mean_reward) * self.soft_probs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.nb_action_taken = 0\n",
    "        self.q = np.zeros(self.nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_agent = GradientBandit(nb_actions=env.action_space.n, alpha=0.1)\n",
    "mean_rewards_gb, percent_optimal_action_gb = run_exp(nb_exps, env, gb_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "egreedy_agent = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1, optimistic_value=0)\n",
    "mean_rewards_egreedy, percent_optimal_action_egreedy = run_exp(nb_exps, env, egreedy_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_exp_4 = {\n",
    "    'GradientBandit Alpha=0.1': {\n",
    "        \"mean_reward\": mean_rewards_gb,\n",
    "        \"optimal_action\": percent_optimal_action_gb,\n",
    "        \"color\": \"blue\"\n",
    "    },\n",
    "    'Egreedy 0.01': {\n",
    "        \"mean_reward\": mean_rewards_egreedy,\n",
    "        \"optimal_action\": percent_optimal_action_egreedy,\n",
    "        \"color\": \"green\"\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_results(\"Gradient Bandit vs Epsilon Greedy\", [\"Average Reward / Steps\", \"Optimal Action / Steps\"], results_exp_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parameter_study(nb_exps, env, agents_parameters):\n",
    "    results_mean_reward = {}\n",
    "    results_percent_optimal_action = {}\n",
    "\n",
    "    for agent_name in agents_parameters:\n",
    "        results_mean_reward[agent_name] = []\n",
    "        results_percent_optimal_action[agent_name] = []\n",
    "\n",
    "    for agent_name in agents_parameters:\n",
    "        print(agent_name)\n",
    "\n",
    "        for parameter in agents_parameters[agent_name][\"parameters\"]:\n",
    "            print(\"    running parameters:\", parameter)\n",
    "\n",
    "            agent = agents_parameters[agent_name][\"class\"](nb_actions=env.action_space.n, **parameter)\n",
    "\n",
    "            mean_reward_over_steps, percent_optimal_action_over_steps = run_exp(nb_exps, env, agent)\n",
    "\n",
    "            mean_reward = np.mean(mean_reward_over_steps)\n",
    "            mean_optimal_action_percent = np.mean(percent_optimal_action_over_steps)\n",
    "\n",
    "            results_mean_reward[agent_name].append(mean_reward)\n",
    "            results_percent_optimal_action[agent_name].append(mean_optimal_action_percent)\n",
    "\n",
    "    return results_mean_reward, results_percent_optimal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_study_results(agents_parameters, results_mean_reward, results_percent_optimal_action):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Mean Reward / Parameters\", \"Mean Optimal Action / Parameters\"])\n",
    "\n",
    "    x = []\n",
    "\n",
    "    for agent_name in results_mean_reward:\n",
    "        parameter = agents_parameters[agent_name][\"variable\"]\n",
    "        x += [p[parameter] for p in agents_parameters[agent_name][\"parameters\"]]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[p[parameter] for p in agents_parameters[agent_name][\"parameters\"]],\n",
    "                       y=results_mean_reward[agent_name], line_color=agents_parameters[agent_name][\"color\"],\n",
    "                       name=agent_name)\n",
    "        , row=1, col=1)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[p[parameter] for p in agents_parameters[agent_name][\"parameters\"]],\n",
    "                       y=results_percent_optimal_action[agent_name], line_color=agents_parameters[agent_name][\"color\"],\n",
    "                       showlegend=False)\n",
    "        , row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Parameter Study\",\n",
    "        legend_title=\"Parameters\",\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        type='category',\n",
    "        tickmode= 'array',\n",
    "        categoryorder= 'array',\n",
    "        categoryarray= sorted(x))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\n",
    "    \"EpsilonGreedy\": {\n",
    "        \"class\": EpsilonGreedy,\n",
    "        \"color\": \"red\",\n",
    "        \"variable\": \"epsilon\",\n",
    "        \"parameters\": [\n",
    "            {\"epsilon\": 1 / 128},\n",
    "            {\"epsilon\": 1 / 64},\n",
    "            {\"epsilon\": 1 / 32},\n",
    "            {\"epsilon\": 1 / 16},\n",
    "            {\"epsilon\": 1 / 8},\n",
    "            {\"epsilon\": 1 / 4}\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"Greedy Optimistic\": {\n",
    "        \"class\": EpsilonGreedy,\n",
    "        \"color\": \"black\",\n",
    "        \"variable\": \"optimistic_value\",\n",
    "        \"parameters\": [\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 1 / 4},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 1 / 2},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 1},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 2},\n",
    "            {\"epsilon\": 0, \"optimistic_value\": 4},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"UCB\": {\n",
    "        \"class\": UpperConfidenceBound,\n",
    "        \"color\": \"blue\",\n",
    "        \"variable\": \"confidence\",\n",
    "        \"parameters\": [\n",
    "            {\"confidence\": 1 / 16},\n",
    "            {\"confidence\": 1 / 8},\n",
    "            {\"confidence\": 1 / 4},\n",
    "            {\"confidence\": 1 / 2},\n",
    "            {\"confidence\": 1},\n",
    "            {\"confidence\": 2},\n",
    "            {\"confidence\": 4},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"Gradient Bandit\": {\n",
    "        \"class\": GradientBandit,\n",
    "        \"color\": \"green\",\n",
    "        \"variable\": \"alpha\",\n",
    "        \"parameters\": [\n",
    "            {\"alpha\": 1 / 32},\n",
    "            {\"alpha\": 1 / 16},\n",
    "            {\"alpha\": 1 / 8},\n",
    "            {\"alpha\": 1 / 4},\n",
    "            {\"alpha\": 1 / 2},\n",
    "            {\"alpha\": 1},\n",
    "            {\"alpha\": 2},\n",
    "        ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 min on my computer (Ryzen 5 5500U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000\n",
    "results_mean_reward, results_percent_optimal_action = run_parameter_study(nb_exps, env, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_study_results(agents, results_mean_reward, results_percent_optimal_action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

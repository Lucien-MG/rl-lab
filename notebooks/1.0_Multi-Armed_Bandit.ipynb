{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use:\n",
    "- Numpy for calculation\n",
    "- gymnasium for rl reproducible envrionment\n",
    "- plotly for graph ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "RL uses training information that evaluates the actions taken ratther than instructs by giving correct actions.\n",
    "This is what creates the need for active exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple version of the k-armed bandit problem is useful because of its nonassociative nature. This is a good problem or environement to learn basic reinforcement learning methodes since it avoids much of the complexity of the full reinforcement learning.\n",
    "\n",
    "So let's create a simple gymnasium environement to re-create the k-armed bandit problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The k-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are faced repeatedly with a choice among\n",
    "k different options, or actions. After each choice you receive a numerical reward chosen\n",
    "from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example,\n",
    "over 1000 action selections, or time steps.\n",
    "\n",
    "Here we use gymnasium for craeting what we call an environement. Gymnasium is an framework that allow to standardize the environment api so that poeple can easily reproduce experimentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit(gym.Env):\n",
    "\n",
    "    def __init__(self, nb_arms=10, nb_steps=1000, mean=0, variance=1, noise_variance=1):\n",
    "        self._nb_arms = nb_arms\n",
    "        self._nb_steps = nb_steps\n",
    "\n",
    "        self._mean = 0\n",
    "        self._noise_mean = 0\n",
    "        self._variance = variance\n",
    "        self._noise_variance = noise_variance\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(nb_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._step += 1\n",
    "    \n",
    "        reward = self._arms[action]\n",
    "        reward_noise = self.np_random.normal(self._noise_mean, self._noise_variance)\n",
    "        terminated = self._step >= self._nb_steps\n",
    "\n",
    "        info = { \"is_optimal_action\": int(action == np.argmax(self._arms)) }\n",
    "\n",
    "        return reward + reward_noise, terminated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._step = 0\n",
    "        self._arms = self.np_random.normal(self._mean, self._variance, size=self._nb_arms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_arms = 10\n",
    "nb_steps=1000\n",
    "\n",
    "env = KArmedBandit(nb_arms=nb_arms, nb_steps=nb_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see if the implementation of the k-armed bandit is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# Sample our distribution to see it's correct\n",
    "data = np.array([[env.step(i)[0] for _ in range(2000)] for i in range(len(env._arms))])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(env._arms)):\n",
    "    fig.add_trace(go.Violin(x=[i] * len(data[i]), y=data[i], name=\"q*(\" + str(i) + \") = \" + str(env._arms[i])[:4], meanline_visible=True))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"K-Armed Bandit Problem Distribution\",\n",
    "    xaxis_title=\"Actions\",\n",
    "    yaxis_title=\"Reward Distributions\",\n",
    "    legend_title=\"True Value of q*(a)\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true value of each q*(a) is near the mean of each distribution, it seems good !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.sum_of_rewards = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.sum_of_rewards / self.nb_action_taken)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.sum_of_rewards[action] += reward\n",
    "        self.nb_action_taken[action] += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.sum_of_rewards = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \"-greedy action selection, for the case of two actions and \" = 0.5, what is\n",
    "the probability that the greedy action is selected?\n",
    "\n",
    "Well, there is a probability of 0.5 to take the greedy action then 0.5 to take a random action; in this case there is a 1/2 chance to take the greedy action.\n",
    "So the answer is 0.5 + (0.5 * 0.5) = 0.75 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = EpsilonGreedy(nb_actions=10, epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent):\n",
    "    list_of_reward = []\n",
    "    list_of_optimal_action = []\n",
    "\n",
    "    env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action()\n",
    "\n",
    "        reward, terminated, info = env.step(action)\n",
    "\n",
    "        agent.observe(action, reward)\n",
    "\n",
    "        list_of_reward.append(reward)\n",
    "        list_of_optimal_action.append(info[\"is_optimal_action\"])\n",
    "    \n",
    "    return np.array(list_of_reward), np.array(list_of_optimal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reward, list_of_optimal_action = run_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(list_of_reward)), y=list_of_reward, mode='lines'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(list_of_optimal_action)), y=list_of_optimal_action, mode='lines', name='Canada'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to see any result here, there is to much noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this for 2000 independent runs,\n",
    "each with a different bandit problem, we obtained measures of the learning algorithm’s\n",
    "average behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(nb_exps, env, agent):\n",
    "    list_rewards, list_optimal_action = run_env(env, agent)\n",
    "\n",
    "    for _ in range(nb_exps - 1):\n",
    "        list_rewards_tmp, list_optimal_action_tmp = run_env(env, agent)\n",
    "\n",
    "        list_rewards += list_rewards_tmp\n",
    "        list_optimal_action += list_optimal_action_tmp\n",
    "\n",
    "    return list_rewards / nb_exps, (list_optimal_action / nb_exps) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_01 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.01)\n",
    "mean_rewards_01, percent_optimal_action_01 = run_exp(nb_exps, env, agent_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1)\n",
    "mean_rewards_1, percent_optimal_action_1 = run_exp(nb_exps, env, agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_greedy = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.0)\n",
    "mean_rewards_0, percent_optimal_action_0 = run_exp(nb_exps, env, agent_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Average Reward / Steps\", \"Optimal Action / Steps\"])\n",
    "\n",
    "x = np.arange(len(mean_rewards_01))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_01, line_color=\"blue\", name='Egreedy 0.01'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_1, line_color=\"red\", name='Egreedy 0.1'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_0, line_color=\"green\", name='Egreedy 0.0 (greedy)'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_01, line_color=\"blue\", showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_1, line_color=\"red\", showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_0, line_color=\"green\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Compares greedy method with different parameters (0.01, 0.1 and 0)\",\n",
    "    legend_title=\"Parameters\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.nb_action_taken[action] += 1\n",
    "        self.q[action] += (reward - self.q[action]) / self.nb_action_taken[action]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_01 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.01)\n",
    "mean_rewards_01, percent_optimal_action_01 = run_exp(nb_exps, env, agent_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1)\n",
    "mean_rewards_1, percent_optimal_action_1 = run_exp(nb_exps, env, agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_greedy = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.0)\n",
    "mean_rewards_0, percent_optimal_action_0 = run_exp(nb_exps, env, agent_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Average Reward / Steps\", \"Optimal Action / Steps\"])\n",
    "\n",
    "x = np.arange(len(mean_rewards_01))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_01, line_color=\"blue\", name='Egreedy 0.01'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_1, line_color=\"red\", name='Egreedy 0.1'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_0, line_color=\"green\", name='Egreedy 0.0 (greedy)'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_01, line_color=\"blue\", showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_1, line_color=\"red\", showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_0, line_color=\"green\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Compares greedy method with different parameters (0.01, 0.1 and 0)\",\n",
    "    legend_title=\"Parameters\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon=0.1, alpha=0.1, optimistic_value=0):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.optimistic_value = optimistic_value\n",
    "\n",
    "        self.q = np.ones(self.nb_actions) * optimistic_value\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.q[action] += self.alpha * (reward - self.q[action])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.ones(self.nb_actions) * self.optimistic_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_optimistic = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.01, alpha=0.1, optimistic_value=5)\n",
    "mean_rewards_optimistic, percent_optimal_action_optimistic = run_exp(nb_exps, env, agent_optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_non_optimistic = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1, alpha=0.1, optimistic_value=0)\n",
    "mean_rewards_non_optimistic, percent_optimal_action_non_optimistic = run_exp(nb_exps, env, agent_non_optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "x = np.arange(len(mean_rewards_optimistic))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_optimistic, line_color=\"blue\", name='Egreedy Optimistic'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_non_optimistic, line_color=\"red\", name='Egreedy Non Optimistic'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_optimistic, line_color=\"blue\", showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_non_optimistic, line_color=\"red\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Compares greedy method with different parameters (0.01, 0.1 and 0)\",\n",
    "    legend_title=\"Parameters\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper-Confidence-Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfidenceBound():\n",
    "\n",
    "    def __init__(self, nb_actions, confidence, alpha=0.1):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.confidence = confidence\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)\n",
    "        self.upper_configdence = np.zeros(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        return np.argmax(self.q + self.upper_configdence)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.q[action] += self.alpha * (reward - self.q[action])\n",
    "\n",
    "        self.nb_action_taken[action] += 1\n",
    "\n",
    "        self.upper_configdence = self.confidence * np.sqrt(np.log(np.sum(self.nb_action_taken)) / self.nb_action_taken)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "        self.nb_action_taken = np.ones(self.nb_actions)\n",
    "        self.upper_configdence = np.zeros(self.nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_condidence_agent = UpperConfidenceBound(nb_actions=env.action_space.n, confidence=2, alpha=0.1)\n",
    "mean_rewards_upper_confidence, percent_optimal_action_upper_confidence = run_exp(2000, env, upper_condidence_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egreedy_agent = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.01, alpha=0.1, optimistic_value=0)\n",
    "mean_rewards_egreedy, percent_optimal_action_egreedy = run_exp(2000, env, egreedy_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "x = np.arange(nb_exps)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_upper_confidence, name='UCB'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_egreedy, name='Egreedy e=0.01'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_upper_confidence, name='UCB'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_egreedy, name='Egreedy e=0.01'), row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Bandit Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(state_action_value):\n",
    "    e_x = np.exp(state_action_value - np.max(state_action_value))\n",
    "    probs = e_x / e_x.sum(axis=0)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBandit():\n",
    "\n",
    "    def __init__(self, nb_actions, alpha):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.alpha = alpha\n",
    "        self.soft_probs = None\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        self.soft_probs = softmax(self.q)\n",
    "        return np.random.choice(self.nb_actions, 1, p=self.soft_probs)[0]\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.soft_probs[action] = - (1 - self.soft_probs[action])\n",
    "        self.q -= self.alpha * (reward - self.q) * self.soft_probs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_agent = GradientBandit(nb_actions=env.action_space.n, alpha=0.1)\n",
    "mean_rewards_gb, percent_optimal_action_gb = run_exp(nb_exps, env, gb_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egreedy_agent = EpsilonGreedy(nb_actions=env.action_space.n, epsilon=0.1, alpha=0.1, optimistic_value=0)\n",
    "mean_rewards_egreedy, percent_optimal_action_egreedy = run_exp(nb_exps, env, egreedy_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Average Reward / Steps\", \"Optimal Action / Steps\"])\n",
    "\n",
    "x = np.arange(len(mean_rewards_egreedy))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_gb, line_color=\"blue\", name='GradientBandit Alpha=0.1'), row=1, col=1)\n",
    "#fig.add_trace(go.Scatter(x=x, y=mean_rewards_gb_2, line_color=\"red\", name='GradientBandit Alpha=0.15'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=mean_rewards_egreedy, line_color=\"green\", name='Egreedy epsilon=0.01'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_gb, line_color=\"blue\", showlegend=False), row=1, col=2)\n",
    "#fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_gb_2, line_color=\"red\", showlegend=False), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=x, y=percent_optimal_action_egreedy, line_color=\"green\", showlegend=False), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Compares greedy method with different parameters (0.01, 0.1 and 0)\",\n",
    "    legend_title=\"Parameters\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parameter_study(nb_exps, env, agents_parameters):\n",
    "    results = {}\n",
    "\n",
    "    for agent_name in agents_parameters:\n",
    "        results[agent_name] = []\n",
    "\n",
    "    for agent_name in agents_parameters:\n",
    "        print(agent_name)\n",
    "\n",
    "        for parameter in agents_parameters[agent_name][\"parameters\"]:\n",
    "            print(\"    running parameters:\", parameter)\n",
    "\n",
    "            agent = agents_parameters[agent_name][\"class\"](nb_actions=env.action_space.n, **parameter)\n",
    "\n",
    "            mean_reward_over_steps, percent_optimal_action_over_steps = run_exp(nb_exps, env, agent)\n",
    "\n",
    "            mean_reward = np.mean(mean_reward_over_steps)\n",
    "            mean_optimal_action_percent = np.mean(percent_optimal_action_over_steps)\n",
    "\n",
    "            results[agent_name].append(mean_reward)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\n",
    "    \"EpsilonGreedy\": {\n",
    "        \"class\": EpsilonGreedy,\n",
    "        \"color\": \"red\",\n",
    "        \"variable\": \"epsilon\",\n",
    "        \"parameters\": [\n",
    "            {\"epsilon\": 1 / 128},\n",
    "            {\"epsilon\": 1 / 64},\n",
    "            {\"epsilon\": 1 / 32},\n",
    "            {\"epsilon\": 1 / 16},\n",
    "            {\"epsilon\": 1 / 8},\n",
    "            {\"epsilon\": 1 / 4}\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"EpsilonGreedy Optimistic\": {\n",
    "        \"class\": EpsilonGreedy,\n",
    "        \"color\": \"black\",\n",
    "        \"variable\": \"optimistic_value\",\n",
    "        \"parameters\": [\n",
    "            {\"epsilon\": 1 / 16, \"optimistic_value\": 1 / 4},\n",
    "            {\"epsilon\": 1 / 16, \"optimistic_value\": 1 / 2},\n",
    "            {\"epsilon\": 1 / 16, \"optimistic_value\": 1},\n",
    "            {\"epsilon\": 1 / 16, \"optimistic_value\": 2},\n",
    "            {\"epsilon\": 1 / 16, \"optimistic_value\": 4},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"UCB\": {\n",
    "        \"class\": UpperConfidenceBound,\n",
    "        \"color\": \"blue\",\n",
    "        \"variable\": \"confidence\",\n",
    "        \"parameters\": [\n",
    "            {\"confidence\": 1 / 16},\n",
    "            {\"confidence\": 1 / 8},\n",
    "            {\"confidence\": 1 / 4},\n",
    "            {\"confidence\": 1 / 2},\n",
    "            {\"confidence\": 1},\n",
    "            {\"confidence\": 2},\n",
    "            {\"confidence\": 4},\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"Gradient Bandit\": {\n",
    "        \"class\": GradientBandit,\n",
    "        \"color\": \"green\",\n",
    "        \"variable\": \"alpha\",\n",
    "        \"parameters\": [\n",
    "            {\"alpha\": 1 / 32},\n",
    "            {\"alpha\": 1 / 16},\n",
    "            {\"alpha\": 1 / 8},\n",
    "            {\"alpha\": 1 / 4},\n",
    "            {\"alpha\": 1 / 2},\n",
    "            {\"alpha\": 1},\n",
    "            {\"alpha\": 2},\n",
    "        ],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17 min on my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_exps = 2000\n",
    "\n",
    "results = run_parameter_study(nb_exps, env, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(agents_parameters, results):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Mean Reward / Parameters\", \"Mean Optimal Action / Parameters\"])\n",
    "\n",
    "    x = []\n",
    "\n",
    "    for agent_name in results:\n",
    "        parameter = agents_parameters[agent_name][\"variable\"]\n",
    "        x += [p[parameter] for p in agents_parameters[agent_name][\"parameters\"]]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[p[parameter] for p in agents_parameters[agent_name][\"parameters\"]],\n",
    "                       y=results[agent_name], line_color=agents_parameters[agent_name][\"color\"],\n",
    "                       name=agent_name)\n",
    "        , row=1, col=1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Parameter Study\",\n",
    "        legend_title=\"Parameters\",\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        type='category',\n",
    "        tickmode= 'array',\n",
    "        categoryorder= 'array',\n",
    "        categoryarray= sorted(x))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(agents, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1', sab=True) # render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_env(env, agent):\n",
    "    terminated = False\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action(observation)\n",
    "\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        agent.observe(observation, reward)\n",
    "\n",
    "        observation = new_observation\n",
    "    \n",
    "    agent.estimating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCFirstVisit():\n",
    "\n",
    "    def __init__(self, gamma, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_value = collections.defaultdict(lambda: 0)\n",
    "        self.returns = collections.defaultdict(lambda: [])\n",
    "\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def observe(self, state, reward):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def estimating(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            self.returns[self.states[t]].append(g)\n",
    "            self.state_value[self.states[t]] = sum(self.returns[self.states[t]]) / len(self.returns[self.states[t]])\n",
    "        \n",
    "        self.states = []\n",
    "        self.rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random policy\n",
    "def random_policy(state):\n",
    "    return np.random.randint(low=0, high=1, size=(1))[0]\n",
    "\n",
    "def stick_policy(state):\n",
    "    player_score = state[0]\n",
    "    if player_score in [20, 21]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 \n",
    "\n",
    "agent = MCFirstVisit(gamma=1, policy=stick_policy)\n",
    "\n",
    "play_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500_000):\n",
    "    play_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent.state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "Z = np.zeros(shape=(22, 12)) * np.nan\n",
    "\n",
    "for k in agent.state_value.keys():\n",
    "    Z[k[0]][k[1]] = agent.state_value[k]\n",
    "\n",
    "sh_0, sh_1 = Z.shape\n",
    "\n",
    "x, y = np.linspace(0, sh_1, sh_1), np.linspace(0, sh_0, sh_0)\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x, y=y)])\n",
    "\n",
    "fig.update_layout(title='MCFirstVisit',\n",
    "                  autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â (Exploring Starts)\n",
    "class MonteCarloES():\n",
    "\n",
    "    def __init__(self, gamma, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_action_values = collections.defaultdict(lambda: [1, 1])\n",
    "        self.returns = collections.defaultdict(lambda: [])\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        state_action_value = self.state_action_values[state]\n",
    "        return self.policy(state_action_value)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def estimating(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            return_state_action_index = str(self.states[t]) + \" \" + str(self.actions[t])\n",
    "            self.returns[return_state_action_index].append(g)\n",
    "            self.state_action_values[self.states[t]][self.actions[t]] = sum(self.returns[return_state_action_index]) / len(self.returns[return_state_action_index])\n",
    "\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_env(env, agent):\n",
    "    terminated = False\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action(observation)\n",
    "\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        agent.observe(observation, action, reward)\n",
    "\n",
    "        observation = new_observation\n",
    "    \n",
    "    agent.estimating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_policy(state_action_value):\n",
    "    return np.argmax(state_action_value)\n",
    "\n",
    "agent = MonteCarloES(gamma=0.9, policy=max_policy)\n",
    "env = gym.make('Blackjack-v1', sab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(150_000):\n",
    "    play_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "Z = np.zeros(shape=(22, 12)) * np.nan\n",
    "\n",
    "for k in agent.state_action_values.keys():\n",
    "    Z[k[0]][k[1]] = np.max(agent.state_action_values[k])\n",
    "\n",
    "sh_0, sh_1 = Z.shape\n",
    "\n",
    "x, y = np.linspace(0, sh_1, sh_1), np.linspace(0, sh_0, sh_0)\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x, y=y)])\n",
    "\n",
    "fig.update_layout(title='MCFirstVisit',\n",
    "                  autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Import the necessaries libraries\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1', sab=True) # render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_env(env, agent):\n",
    "    terminated = False\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action(observation)\n",
    "\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        agent.observe(observation, action, reward)\n",
    "\n",
    "        observation = new_observation\n",
    "    \n",
    "    agent.estimating()\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCFirstVisit():\n",
    "\n",
    "    def __init__(self, gamma, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_value = collections.defaultdict(lambda: 0)\n",
    "        self.returns = collections.defaultdict(lambda: [])\n",
    "\n",
    "        self.states = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def estimating(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            self.returns[self.states[t]].append(g)\n",
    "\n",
    "            # Here, we could avoid keeping a list a return values and use the update formula\n",
    "            self.state_value[self.states[t]] = sum(self.returns[self.states[t]]) / len(self.returns[self.states[t]])\n",
    "        \n",
    "        self.states = []\n",
    "        self.rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.random.randint(low=0, high=1, size=(1))[0]\n",
    "\n",
    "def stick_policy(state):\n",
    "    player_score = state[0]\n",
    "    if player_score in [20, 21]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 \n",
    "\n",
    "agent = MCFirstVisit(gamma=1, policy=stick_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100_000):\n",
    "    play_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent.state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.zeros(shape=(22, 12)) * np.nan\n",
    "\n",
    "for k in agent.state_value.keys():\n",
    "    Z[k[0]][k[1]] = agent.state_value[k]\n",
    "\n",
    "sh_0, sh_1 = Z.shape\n",
    "\n",
    "x, y = np.linspace(0, sh_1, sh_1), np.linspace(0, sh_0, sh_0)\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x, y=y)])\n",
    "\n",
    "fig.update_layout(title='MCFirstVisit',\n",
    "                  autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloGeneralizePolicyIteration():\n",
    "\n",
    "    def __init__(self, action_space, gamma, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_action_values = collections.defaultdict(action_space)\n",
    "        self.returns = collections.defaultdict(lambda: [])\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        state_action_value = self.state_action_values[state]\n",
    "        return self.policy(state_action_value)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def estimating(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "\n",
    "            return_state_action_index = str(self.states[t]) + \" \" + str(self.actions[t])\n",
    "            self.returns[return_state_action_index].append(g)\n",
    "            self.state_action_values[self.states[t]][self.actions[t]] = sum(self.returns[return_state_action_index]) / len(self.returns[return_state_action_index])\n",
    "\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_policy(state_action_value):\n",
    "    return np.argmax(state_action_value)\n",
    "\n",
    "def build_action_space_exploring_start(env):\n",
    "    return lambda: [1] * env.action_space.n\n",
    "\n",
    "agent = MonteCarloGeneralizePolicyIteration(action_space=build_action_space_exploring_start(env), gamma=1, policy=max_policy)\n",
    "env = gym.make('Blackjack-v1', sab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000\n",
    "mean_last_100_rewards = []\n",
    "last_100_rewards = [0] * buffer_size\n",
    "\n",
    "for i in range(150_000):\n",
    "    last_reward = play_env(env, agent)\n",
    "\n",
    "    last_100_rewards[i % buffer_size] = last_reward\n",
    "\n",
    "    if i % buffer_size == 0:\n",
    "        mean_last_100_rewards.append(sum(last_100_rewards) / buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.zeros(shape=(22, 12)) * np.nan\n",
    "\n",
    "for k in agent.state_action_values.keys():\n",
    "    Z[k[0]][k[1]] = np.max(agent.state_action_values[k])\n",
    "\n",
    "sh_0, sh_1 = Z.shape\n",
    "\n",
    "x, y = np.linspace(0, sh_1, sh_1), np.linspace(0, sh_0, sh_0)\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x, y=y)])\n",
    "\n",
    "fig.update_layout(title='MonteCarloExploringStart',\n",
    "                  autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[i for i in range(len(mean_last_100_rewards))],\n",
    "    y=mean_last_100_rewards,\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â (Exploring Starts)\n",
    "class MonteCarloES():\n",
    "\n",
    "    def __init__(self, gamma, alpha, policy):\n",
    "        self.gamma  = gamma\n",
    "        self.alpha = alpha\n",
    "        self.policy = policy\n",
    "\n",
    "        self.state_action_values = collections.defaultdict(lambda: [1, 1])\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def action(self, state):\n",
    "        state_action_value = self.state_action_values[state]\n",
    "        return self.policy(state_action_value)\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def estimating(self):\n",
    "        g = 0\n",
    "\n",
    "        for t in reversed(range(len(self.states))):\n",
    "            g = self.gamma * g + self.rewards[t]\n",
    "            self.state_action_values[self.states[t]][self.actions[t]] += self.alpha * (g - self.state_action_values[self.states[t]][self.actions[t]])\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_policy(state_action_value):\n",
    "    return np.argmax(state_action_value)\n",
    "\n",
    "agent = MonteCarloES(gamma=0.9, alpha=0.05, policy=max_policy)\n",
    "env = gym.make('Blackjack-v1', sab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000\n",
    "mean_last_100_rewards = []\n",
    "last_100_rewards = [0] * buffer_size\n",
    "\n",
    "for i in range(500_000):\n",
    "    last_reward = play_env(env, agent)\n",
    "\n",
    "    last_100_rewards[i % buffer_size] = last_reward\n",
    "\n",
    "    if i >= buffer_size:\n",
    "        mean_last_100_rewards.append(sum(last_100_rewards) / buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "Z = np.zeros(shape=(22, 12)) * np.nan\n",
    "\n",
    "for k in agent.state_action_values.keys():\n",
    "    Z[k[0]][k[1]] = np.argmax(agent.state_action_values[k])\n",
    "\n",
    "sh_0, sh_1 = Z.shape\n",
    "\n",
    "x, y = np.linspace(0, sh_1, sh_1), np.linspace(0, sh_0, sh_0)\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x, y=y)])\n",
    "\n",
    "fig.update_layout(title='MCFirstVisit',\n",
    "                  autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[i for i in range(len(mean_last_100_rewards))],\n",
    "    y=mean_last_100_rewards,\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

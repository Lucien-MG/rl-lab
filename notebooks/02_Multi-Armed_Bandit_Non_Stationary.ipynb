{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple version of the k-armed bandit problem is useful because of its nonassociative nature. This is a good problem or environement to learn basic reinforcement learning methodes.\n",
    "\n",
    "So let's create a simple gymnasium environement to re-create the k-armed bandit problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The k-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are faced repeatedly with a choice among\n",
    "k di↵erent options, or actions. After each choice you receive a numerical reward chosen\n",
    "from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example,\n",
    "over 1000 action selections, or time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBanditNonStationary(gym.Env):\n",
    "\n",
    "    def __init__(self, nb_arms=10, nb_steps=10_000):\n",
    "        self._nb_arms = nb_arms\n",
    "        self._nb_steps = nb_steps\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(nb_arms)\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._step += 1\n",
    "    \n",
    "        reward = self._arms[action]\n",
    "        reward_noise = self.np_random.normal(0, 1, size=1)[0]\n",
    "        terminated = self._step >= self._nb_steps\n",
    "\n",
    "        info = { \"is_optimal_action\": int(action == np.argmax(self._arms)) }\n",
    "\n",
    "        # Derivation\n",
    "        self._arms += self.np_random.normal(0, 0.01, size=self._nb_arms)\n",
    "\n",
    "        return reward + reward_noise, terminated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._step = 0\n",
    "        self._arms = self.np_random.normal(0, 1, size=self._nb_arms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see if the implementation of the k-armed bandit is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KArmedBanditNonStationary(nb_arms=10)\n",
    "env.reset()\n",
    "\n",
    "# Sample our distribution to see it's correct\n",
    "data = np.array([[env.step(i)[0] for i in range(len(env._arms))] for _ in range(200)])\n",
    "index = [i for i in range(len(env._arms))]\n",
    "\n",
    "plt.violinplot(data, index, showmeans=True)\n",
    "\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems good !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "\n",
    "    def __init__(self, nb_actions, epsilon, alpha):\n",
    "        self.nb_actions = nb_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.q = np.zeros(self.nb_actions)\n",
    "\n",
    "    def action(self):\n",
    "        take_random_action_prob = np.random.uniform(0, 1)\n",
    "\n",
    "        if take_random_action_prob < self.epsilon:\n",
    "            return np.random.randint(0, self.nb_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q)\n",
    "    \n",
    "    def observe(self, action, reward):\n",
    "        self.q[action] += self.alpha * (reward - self.q[action])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q = np.zeros(self.nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KArmedBanditNonStationary(nb_arms=10)\n",
    "agent = EpsilonGreedy(nb_actions=10, epsilon=0.01, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent):\n",
    "    list_of_reward = []\n",
    "    list_of_optimal_action = []\n",
    "\n",
    "    env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        action = agent.action()\n",
    "\n",
    "        reward, terminated, info = env.step(action)\n",
    "\n",
    "        agent.observe(action, reward)\n",
    "\n",
    "        list_of_reward.append(reward)\n",
    "        list_of_optimal_action.append(info[\"is_optimal_action\"])\n",
    "    \n",
    "    return np.array(list_of_reward), np.array(list_of_optimal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reward, list_of_optimal_action = run_env(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_of_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_of_optimal_action)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to see any result here, there is to much noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this for 2000 independent runs,\n",
    "each with a di↵erent bandit problem, we obtained measures of the learning algorithm’s\n",
    "average behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(nb_exps, env, agent):\n",
    "    list_rewards, list_optimal_action = run_env(env, agent)\n",
    "\n",
    "    for _ in range(nb_exps - 1):\n",
    "        list_rewards_tmp, list_optimal_action_tmp = run_env(env, agent)\n",
    "\n",
    "        list_rewards += list_rewards_tmp\n",
    "        list_optimal_action += list_optimal_action_tmp\n",
    "\n",
    "    return list_rewards / nb_exps, (list_optimal_action / nb_exps) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KArmedBanditNonStationary(nb_arms=10)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_01 = EpsilonGreedy(nb_actions=10, epsilon=0.01, alpha=0.1)\n",
    "mean_rewards_01, percent_optimal_action_01 = run_exp(2000, env, agent_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = EpsilonGreedy(nb_actions=10, epsilon=0.1, alpha=0.1)\n",
    "mean_rewards_1, percent_optimal_action_1 = run_exp(2000, env, agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_greedy = EpsilonGreedy(nb_actions=10, epsilon=0.0, alpha=0.1)\n",
    "mean_rewards_0, percent_optimal_action_0 = run_exp(2000, env, agent_greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards_01, color=\"tab:red\", label='Egreedy 0.01')\n",
    "\n",
    "plt.plot(mean_rewards_1, color=\"tab:blue\", label='Egreedy 0.1')\n",
    "\n",
    "plt.plot(mean_rewards_0, color=\"tab:green\", label='Egreedy 0.0 (greedy)')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(percent_optimal_action_01, color=\"tab:red\", label='Egreedy 0.01')\n",
    "\n",
    "plt.plot(percent_optimal_action_1, color=\"tab:blue\", label='Egreedy 0.1')\n",
    "\n",
    "plt.plot(percent_optimal_action_0, color=\"tab:green\", label='Egreedy 0.0 (greedy)')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('% Optimal Action')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
